/*
 * Copyright (c) 2017, The Regents of the University of California (Regents).
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are
 * met:
 *
 *    1. Redistributions of source code must retain the above copyright
 *       notice, this list of conditions and the following disclaimer.
 *
 *    2. Redistributions in binary form must reproduce the above
 *       copyright notice, this list of conditions and the following
 *       disclaimer in the documentation and/or other materials provided
 *       with the distribution.
 *
 *    3. Neither the name of the copyright holder nor the names of its
 *       contributors may be used to endorse or promote products derived
 *       from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS AS IS
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 *
 * Please contact the author(s) of this library if you have any questions.
 * Authors: David Fridovich-Keil   ( dfk@eecs.berkeley.edu )
 */

///////////////////////////////////////////////////////////////////////////////
//
// Defines the LinearActionValueFunctor class, which derives from the
// ContinuousActionValueFunctor base class. This class models the value function
// as a linear function of some feature vector generated by the state/action.
//
///////////////////////////////////////////////////////////////////////////////

#ifndef RL_VALUE_LINEAR_ACTION_VALUE_FUNCTOR_H
#define RL_VALUE_LINEAR_ACTION_VALUE_FUNCTOR_H

#include <value/continuous_action_value_functor.hpp>
#include <util/types.hpp>

#include <glog/logging.h>
#include <random>

namespace rl {

  template<typename StateType, typename ActionType>
  class LinearActionValueFunctor :
    public ContinuousActionValueFunctor<StateType, ActionType> {
  public:
    // Constructor/destructor.
    ~LinearActionValueFunctor() {}
    LinearActionValueFunctor();

    // Pure virtual method to output the value at a state/action pair.
    double operator()(const StateType& state, const ActionType& action) const;

    // Pure virtual method to do a gradient update to underlying weights.
    // Returns the average loss.
    double Update(const std::vector<StateType>& states,
                  const std::vector<ActionType>& actions,
                  const std::vector<double>& targets, double step_size);

    // Choose an optimal action in the given state. Returns whether or not
    // optimization was successful.
    bool OptimalAction(const StateType& state, ActionType& action) const;

  private:
    // Vectors of weights.
    VectorXd state_weights_;
    VectorXd action_weights_;
    double bias_;

    // Private helper to compute function output.
    double Evaluate(const StateType& state, const ActionType& action) const;
  }; //\struct LinearStateValueFunctor

// ----------------------------- IMPLEMENTATION ----------------------------- //

  template<typename StateType, typename ActionType>
  LinearActionValueFunctor<StateType, ActionType>::
  LinearActionValueFunctor()
    : state_weights_(VectorXd::Zero(StateType::FeatureDimension())),
      action_weights_(VectorXd::Zero(ActionType::FeatureDimension())),
      bias_(0.0),
      ContinuousActionValueFunctor<StateType, ActionType>() {
    // Create a random number generator for a normal distribution of mean
    // 0.0 and standard deviation 0.1.
    std::random_device rd;
    std::default_random_engine rng(rd());
    std::normal_distribution<double> gaussian(0.0, 0.1);

    // Populate weights from this distribution.
    for (size_t ii = 0; ii < state_weights_.size(); ii++)
      state_weights_(ii) = gaussian(rng);

    for (size_t ii = 0; ii < action_weights_.size(); ii++)
      action_weights_(ii) = gaussian(rng);

    bias_ = gaussian(rng);
  }

  // Pure virtual method to output the value at a state/action pair.
  template<typename StateType, typename ActionType>
  double LinearActionValueFunctor<StateType, ActionType>::
  operator()(const StateType& state, const ActionType& action) const {
    return Evaluate(state, action);
  }

  // Pure virtual method to do a gradient update to underlying weights.
  // Returns the average loss.
  template<typename StateType, typename ActionType>
  double LinearActionValueFunctor<StateType, ActionType>::
  Update(const std::vector<StateType>& states,
         const std::vector<ActionType>& actions,
         const std::vector<double>& targets, double step_size) {
    CHECK_EQ(states.size(), actions.size());
    CHECK_EQ(states.size(), targets.size());

    VectorXd state_gradient = VectorXd::Zero(state_weights_.size());
    VectorXd action_gradient = VectorXd::Zero(action_weights_.size());
    double bias_derivative = 0.0;

    // Loop over all state/action/target triples and update gradients.
    double loss = 0.0;
    for (size_t ii = 0; ii < states.size(); ii++) {
      // Extract feature from state and action.
      VectorXd state_features(state_weights_.size());
      states[ii].Features(state_features);

      VectorXd action_features(action_weights_.size());
      actions[ii].Features(action_features);

      // Compute output.
      const double output = bias_ +
        state_features.dot(state_weights_) +
        action_features.dot(action_weights_);

      // Update gradients.
      const double error = output - targets[ii];
      state_gradient += error * state_features;
      action_gradient += error * action_features;
      bias_derivative += error;

      loss += 0.5 * error * error;
    }

    // Update.
    state_weights_ -= state_gradient * step_size;
    action_weights_ -= action_gradient * step_size;
    bias_ -= bias_derivative * step_size;

    return loss / static_cast<double>(states.size());
  }

  // Choose an optimal action in the given state. Returns whether or not
  // optimization was successful.
  template<typename StateType, typename ActionType>
  bool LinearActionValueFunctor<StateType, ActionType>::
  OptimalAction(const StateType& state, ActionType& action) const {
    VectorXd optimal_features(action_weights_.size());

    // In this case, the optimal action lies at a vertex of the feasible set
    // (assume that the feasible set of actions is a box, i.e. each dimension
    // has an interval constraint).
    for (size_t ii = 0; ii < action_weights_.size(); ii++) {
      if (action_weights_(ii) > 0)
        optimal_features(ii) = ActionType::MaxAlongDimension(ii);
      else
        optimal_features(ii) = ActionType::MinAlongDimension(ii);
    }

    action.FromFeatures(optimal_features);
    return true;
  }

  // Private helper to compute function output.
  template<typename StateType, typename ActionType>
  double LinearActionValueFunctor<StateType, ActionType>::
  Evaluate(const StateType& state, const ActionType& action) const {
    // Extract feature from state and action.
    VectorXd state_features(state_weights_.size());
    state.Features(state_features);

    VectorXd action_features(action_weights_.size());
    action.Features(action_features);

    // Compute inner product.
    const double inner_product = bias_ +
      state_features.dot(state_weights_) +
      action_features.dot(action_weights_);
    return inner_product;
  }

}  //\namespace rl

#endif
